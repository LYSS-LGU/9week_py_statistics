{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a953f79",
   "metadata": {},
   "source": [
    "# 규제 선형 모델 Regularization\n",
    "\n",
    "25.07.22 (화) 11주 49일차\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cc2a8",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "![규제선형모델1](images\\규제선형모델1.png)\n",
    "\n",
    "규제는 과적합 모델(Overfitting Model)을 일반화된 모델(Generalization)로 변환하여 모델이 학습 데이터에 지나치게 맞춰지는 것을 방지하고, 새로운 데이터에도 잘 작동하도록 도와준다.  규제는 모델의 복잡도를 제한하여 더 안정적이고 일반화된 모델을 만들며, 이는 예측 성능을 향상시키는 데 중요한 역할을 한다. \n",
    "\n",
    "- 선형 모델의 비용함수는 RSS, 즉 실제 값과 예측값의 차이를 최소화하는 것을 고려한 방식이다.  이 경우 학습 데이터에 지나치게 맞추게 되어 회귀 계수가 쉽게 커지고, 변동성이 오히려 커져서 테스트 데이터 세트에서는 예측 성능이 저하되기 쉬운 과적합이 우려되었다.\n",
    "- 하지만 **손실 함수만 최소화**하면, 훈련 데이터에 과도하게 적합하는 **과적합(overfitting)** 이 발생할 수 있음.\n",
    "- 방지를 위해 **정규화(regularization)** 를 도입하여 **계수(가중치)의 크기에 제약(penalty)** 을 부여함.\n",
    "- 이 제약 항에 **알파(𝛼)** 를 곱해서 손실 함수와의 균형을 조절함.\n",
    "\n",
    "총 Cost=Loss+α⋅Penalty $= \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} w_j^2$\n",
    "\n",
    "규제는 크게 L2 방식과 L1 방식으로 나눈다.  ****\n",
    "\n",
    "**L2 규제를 적용한 릿지 회귀, L1 규제를 적용한 라쏘 회귀,  두 가지 규제를 합한 것이 엘라스틱넷**이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e47d3",
   "metadata": {},
   "source": [
    "# Ridge  & Lasso\n",
    "\n",
    "**Ridge와 Lasso**는 기존 다중 선형 회귀 모델의 손실 함수에 계수 크기에 대한 **페널티를 더한 함수**를 사용하여 모델을 학습한다.\n",
    "\n",
    "![Ridge&Lasso](images\\Ridge&Lasso.png)\n",
    "\n",
    "## Ridge(L2 규제) 회귀\n",
    "\n",
    "![Ridge(L2규제)회귀](images\\Ridge(L2규제)회귀.png)\n",
    "\n",
    "이런  비용함수를 사용하는 회귀 함수를 **릿지 회귀Ridge Regression** 또는 능형 회귀라고 한다.   \n",
    "\n",
    "알파는 사용자가 정의한 하이퍼파라미터로 모델을 얼마나 규제할지 그 정도를 조절하는 일을 한다. 만일 알파가 0의 값을 가지면 두번째 항 전체가 0이 되므로 원래 비용함수와 동일할 것이다.  반대로 알파가 매우 큰 값을 가지면 모든 파라미터가 거의 0에 가까워 져서 결국 데이터의 평균을 지나는 수평선이 될 것이다.(과소적합)  **하지만 릿지 회귀의 경우 회귀 계수를 0으로 만들지는 않는다.**\n",
    "\n",
    "![Ridge(L2규제)회귀2](images\\Ridge(L2규제)회귀2.png)\n",
    "\n",
    "**규제가 강해질수록 (α 증가)** → 예측 오차/RSS 증가 (덜 민감하게 됨) →  **일부 입력 변수의 계수가 0에 가까워져서 사실상 모델이 사용하는 입력변수의 수가 줄어드는 효과**\n",
    "\n",
    "릿지 회귀는 전체 계수의 크기를 최대한 작게(분산을 줄임) 만드는 동시에 회귀모델의 성능을 올리게 된다. \n",
    "\n",
    "입력변수가 완전히 제거되진 않지만, **영향력이 매우 작아져 사실상 '무시'되는 수준**으로 감소한다.\n",
    " \n",
    "그래서 Ridge의 훈련 데이터는 전체적으로 선형 회귀의 점수보다 낮으나 **데이터가 충분히 많으면 규제항은 덜 중요해져서 릿지 회귀와 선형 회귀의 성능은 같아지게 된다**.  그래서 벌칙 penalty 라고 부르기도 한다.\n",
    "\n",
    "![Ridge(L2규제)회귀3](images\\Ridge(L2규제)회귀3.png)\n",
    "\n",
    "<aside>\n",
    "💡\n",
    "\n",
    "**1) 현재 데이터를 조금 덜 훌륭하게 설명하는 모델을 얻게 되더라도**, \n",
    "\n",
    "**2) 미래 데이터에 대한 예측 성능이 조금 더 좋은 모델**을 만든다.\n",
    "\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692e7c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 3. 릿지 회귀\u001b[39;00m\n\u001b[32m      4\u001b[39m ridge = Ridge(alpha=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# alpha는 규제 강도\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ridge.fit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[32m      6\u001b[39m pred_ridge = ridge.predict(X_test)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[릿지 회귀]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 3. 릿지 회귀\n",
    "ridge = Ridge(alpha=1.0)  # alpha는 규제 강도\n",
    "ridge.fit(X_train, y_train)\n",
    "pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"\\n[릿지 회귀]\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred_ridge))\n",
    "print(\"R2:\", r2_score(y_test, pred_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277baeb",
   "metadata": {},
   "source": [
    "# 규제 회귀모델 (Regularization Regression Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73119544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성 (Ridge 회귀 실습용)\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예시 데이터 생성\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)\n",
    "\n",
    "# train/test 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"✅ 데이터 준비 완료!\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"학습 데이터: {X_train.shape}\")  \n",
    "print(f\"테스트 데이터: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge 회귀 실행\n",
    "ridge = Ridge(alpha=1.0)  # alpha는 규제 강도\n",
    "ridge.fit(X_train, y_train)\n",
    "pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"🔍 [릿지 회귀 결과]\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred_ridge))\n",
    "print(\"R²:\", r2_score(y_test, pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd529d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[릿지 회귀]\n",
      "MSE: 127.60013070944176\n",
      "R2: 0.9978774507478436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Ridge 모델을 만들기 전에 이 코드를 추가해주세요!\n",
    "\n",
    "# --- 데이터 분리 코드 ---\n",
    "# 가지고 있는 전체 데이터(X)와 전체 정답(y)을 넣습니다.\n",
    "# test_size=0.2는 전체 데이터 중 20%를 테스트용으로 사용하겠다는 의미입니다.\n",
    "# random_state는 코드를 다시 실행해도 항상 똑같은 방식으로 데이터를 나누기 위해 설정하는 값입니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 이제 원래 코드를 실행하면 정상적으로 작동합니다 ---\n",
    "# 3. 릿지 회귀\n",
    "ridge = Ridge(alpha=1.0) # alpha는 규제 강도\n",
    "ridge.fit(X_train, y_train) # 이제 X_train이 존재하므로 에러가 발생하지 않습니다.\n",
    "pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"\\n[릿지 회귀]\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred_ridge))\n",
    "print(\"R2:\", r2_score(y_test, pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08249bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127.60013070944176, 0.9978774507478436)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, pred_ridge)\n",
    "r2  = r2_score(y_test, pred_ridge)\n",
    "mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61713dbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1467610003.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m다항 회귀 모델\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "다항 회귀 모델\n",
    "15.5, 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8c44902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4d6b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.fit(X_train, y_train)\n",
    "ridge_preds = ridge_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a09d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge cv mse: 102.793230, r2 : 0.998290\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ridge_mse = mean_squared_error(y_test, ridge_preds)\n",
    "ridge_r2 = r2_score(y_test, ridge_preds)\n",
    "print(f'ridge cv mse: {ridge_mse:4f}, r2 : {ridge_r2:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f179bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbcbcb",
   "metadata": {},
   "source": [
    "## Lasso(L1 규제) 회귀\n",
    "\n",
    "L1  규제는 w의 절대 값에 패널티를 부여하는 방식이다.  \n",
    "\n",
    "- 라쏘 회귀는 전체 입력변수 계수 중 일부를 0으로 만들어 입력 변수를 선택하게 한다.\n",
    "- 일반적으로는 Ridge를 선택하지만, 특성이 굉장히 많고 그 중 일부만 중요한 특성인 경우 또한 일부 특성만으로 분석하기 쉬운 모델을 원할 경우 선택하는 것이 라쏘 회귀이다.\n",
    "- Feature Engineering의 **특성 추출(축소)**와 유사한 기능이라고 생각할 수 있다.\n",
    "\n",
    "![Lasso1](images\\Lasso1.png)\n",
    "\n",
    "![Lasso2](images\\Lasso2.png)\n",
    "\n",
    "\n",
    "- **가중치 𝝏의 값이 커질수록 규제는 강해지며 모델은 일반화된다.**\n",
    "![Lasso3](images\\Lasso3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e9c88c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. 라쏘 회귀\u001b[39;00m\n\u001b[32m      4\u001b[39m lasso = Lasso(alpha=\u001b[32m0.1\u001b[39m)  \u001b[38;5;66;03m# alpha 값 작으면 규제 약해짐\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lasso.fit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[32m      6\u001b[39m pred_lasso = lasso.predict(X_test)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[라쏘 회귀]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# 4. 라쏘 회귀\n",
    "\n",
    "lasso = Lasso(alpha=0.1)  # alpha 값 작으면 규제 약해짐\n",
    "lasso.fit(X_train, y_train)\n",
    "pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"\\n[라쏘 회귀]\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred_lasso))\n",
    "print(\"R2:\", r2_score(y_test, pred_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767954a1",
   "metadata": {},
   "source": [
    "## 리지와 라쏘의 비교\n",
    "\n",
    "![ridgeVSlasso](images\\ridgeVSlasso.png)\n",
    "\n",
    "- Ridge 회귀 계수들이 0을 중심으로 분포하며, 알파값이 커질수록 계수들이 0에 더 가까워지는 경향\n",
    "- Lasso 회귀는 일부 계수들이 정확히 0이 되는 것을 확인할 수 있다.  알파값이 커질수록 더 많은 계수들이 0이 되어 변수 선택 효과를 낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf2047",
   "metadata": {},
   "source": [
    "## Lasso alpha=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3ea29",
   "metadata": {},
   "source": [
    "## LassoCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3d9f6",
   "metadata": {},
   "source": [
    "## 엘라스틱 넷 회귀\n",
    "\n",
    "엘라스틱 넷 Elastic Net 회귀는 L2 규제와 L1 규제를 결합한 회귀이다.  \n",
    "\n",
    "- Lasso(L1): 변수 선택(특정 계수를 0으로 만들어 모델을 간결하게 만듦)에 강함.그러나, 다중공선성이 높은 데이터(독립 변수 간 상관관계가 높은 경우)에서는 불안정할 수 있음.\n",
    "- Ridge(L2): 다중공선성이 높은 데이터에서 안정적이지만, 모든 계수를 0에 가깝게 줄일 뿐 0으로 만들지는 않음.\n",
    "\n",
    "**Elastic Net**은 L1과 L2 규제를 동시에 사용하여 **Lasso의 희소성(sparsity)**과 **Ridge의 안정성(stability)**을 모두 얻는 것을 목표로 한다.\n",
    "\n",
    "![엘라스틱넷회귀](images\\엘라스틱넷회귀.png)\n",
    "\n",
    "- α:  전체 규제의 강도.\n",
    "- ρ: L1과 L2의 가중치를 조절 (0이면 Ridge, 1이면 Lasso).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738f5040",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ElasticNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 5. 엘라스틱넷 회귀\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_enet = \u001b[43mElasticNet\u001b[49m(alpha=\u001b[32m0.1\u001b[39m, l1_ratio=\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# l1_ratio=0.5: L1과 L2의 비중 50:50\u001b[39;00m\n\u001b[32m      3\u001b[39m model_enet.fit(X_train, y_train)\n\u001b[32m      4\u001b[39m pred_enet = model_enet.predict(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'ElasticNet' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. 엘라스틱넷 회귀\n",
    "model_enet = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio=0.5: L1과 L2의 비중 50:50\n",
    "model_enet.fit(X_train, y_train)\n",
    "pred_enet = model_enet.predict(X_test)\n",
    "\n",
    "print(\"\\n[엘라스틱넷 회귀]\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred_enet))\n",
    "print(\"R2:\", r2_score(y_test, pred_enet))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
